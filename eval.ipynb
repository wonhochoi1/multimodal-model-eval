{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Model Evaluation Notebook\n",
    "\n",
    "This notebook provides an interactive interface for running multimodal model evaluations.\n",
    "\n",
    "## Features:\n",
    "- Specify device (CPU/GPU/local)\n",
    "- Choose model and adapter type\n",
    "- Configure evaluation parameters\n",
    "- View results interactively\n",
    "- Compare different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# Import our evaluation components\n",
    "from agent.runner import LocalAgentRunner\n",
    "from agent.adapters.registry import get_adapter\n",
    "from orchestrator.cli import create_unique_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Configuration\n",
    "DEVICE_CONFIG = {\n",
    "    \"device\": \"cpu\",  # \"cpu\", \"cuda\", \"mps\" (for Apple Silicon)\n",
    "    \"platform\": \"local\",  # \"local\", \"edge\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model to run and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"model_name\": \"microsoft/DialoGPT-medium\",\n",
    "    \"adapter_type\": \"huggingface\",  # \"huggingface\", \"vlm_example\", \"custom\"\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_CONFIG = {\n",
    "    \"warmup_trials\": 2,\n",
    "    \"num_trials\": 5,\n",
    "    \"sample_rate_hz\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_CONFIG = {\n",
    "    \"type\": \"vlm_caption\",  # \"vlm_caption\", \"image_classification\", \"asr\"\n",
    "    \"prompt\": \"Describe what you see in this image\",\n",
    "    \"image_path\": \"/mock/path/to/test_image.jpg\",  # Mock path for local testing\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Evaluation Suite\n",
    "Create the suite configuration from the parameters above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation suite created:\n",
      "{\n",
      "  \"name\": \"notebook_eval_20250814_233152\",\n",
      "  \"description\": \"Interactive notebook evaluation\",\n",
      "  \"device\": \"cpu\",\n",
      "  \"platform\": \"local\",\n",
      "  \"adapter_config\": {\n",
      "    \"adapter_type\": \"huggingface\",\n",
      "    \"model_name\": \"microsoft/DialoGPT-medium\",\n",
      "    \"max_new_tokens\": 50,\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.7,\n",
      "    \"device\": \"cpu\"\n",
      "  },\n",
      "  \"run\": {\n",
      "    \"warmup\": 2,\n",
      "    \"repeats\": 5,\n",
      "    \"sample_rate_hz\": 2\n",
      "  },\n",
      "  \"tasks\": [\n",
      "    {\n",
      "      \"id\": \"vlm_caption_task\",\n",
      "      \"type\": \"vlm_caption\",\n",
      "      \"prompt\": \"Describe what you see in this image\",\n",
      "      \"image\": \"/mock/path/to/test_image.jpg\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def create_evaluation_suite() -> Dict[str, Any]:\n",
    "    suite = {\n",
    "        \"name\": f\"notebook_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        \"description\": \"Interactive notebook evaluation\",\n",
    "        \"device\": DEVICE_CONFIG[\"device\"],\n",
    "        \"platform\": DEVICE_CONFIG[\"platform\"],\n",
    "        \n",
    "        \"adapter_config\": {\n",
    "            \"adapter_type\": MODEL_CONFIG[\"adapter_type\"],\n",
    "            \"model_name\": MODEL_CONFIG[\"model_name\"],\n",
    "            \"max_new_tokens\": MODEL_CONFIG[\"max_new_tokens\"],\n",
    "            \"do_sample\": MODEL_CONFIG[\"do_sample\"],\n",
    "            \"temperature\": MODEL_CONFIG[\"temperature\"],\n",
    "            \"device\": DEVICE_CONFIG[\"device\"],\n",
    "        },\n",
    "        \n",
    "        \"run\": {\n",
    "            \"warmup\": EVAL_CONFIG[\"warmup_trials\"],\n",
    "            \"repeats\": EVAL_CONFIG[\"num_trials\"],\n",
    "            \"sample_rate_hz\": EVAL_CONFIG[\"sample_rate_hz\"],\n",
    "        },\n",
    "        \n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"id\": f\"{TASK_CONFIG['type']}_task\",\n",
    "                \"type\": TASK_CONFIG[\"type\"],\n",
    "                \"prompt\": TASK_CONFIG[\"prompt\"],\n",
    "                \"image\": TASK_CONFIG[\"image_path\"],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return suite\n",
    "\n",
    "suite_config = create_evaluation_suite()\n",
    "print(\"Evaluation suite created:\")\n",
    "print(json.dumps(suite_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "Execute the evaluation suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval: notebook_eval_20250814_233152\n",
      "Device: cpu\n",
      "Model: microsoft/DialoGPT-medium\n",
      "Trials: 5\n",
      "\n",
      "Loading adapter...\n",
      "✅ Loaded adapter: HuggingFaceVLMAdapter with config: {'adapter_type': 'huggingface', 'model_name': 'microsoft/DialoGPT-medium', 'max_new_tokens': 50, 'do_sample': True, 'temperature': 0.7, 'device': 'cpu'}\n",
      "\n",
      "Creating runner...\n",
      "\n",
      "⚡ Running evaluation suite...\n",
      "Running 2 warmup trials...\n",
      "  Warmup 1/2\n",
      "Running trial 41282370:\n",
      "  Trial completed: 267.1ms\n",
      "  Warmup 2/2\n",
      "Running trial d99c3197:\n",
      "  Trial completed: 145.3ms\n",
      "Warmup completed\n",
      "\n",
      "Task: vlm_caption_task\n",
      "  Trial 1/5\n",
      "Running trial dfa119ca:\n",
      "  Trial completed: 159.2ms\n",
      "  Trial 2/5\n",
      "Running trial 2569e6fe:\n",
      "  Trial completed: 157.4ms\n",
      "  Trial 3/5\n",
      "Running trial 94b6f5de:\n",
      "  Trial completed: 165.5ms\n",
      "  Trial 4/5\n",
      "Running trial ff036aff:\n",
      "  Trial completed: 147.7ms\n",
      "  Trial 5/5\n",
      "Running trial 31183241:\n",
      "  Trial completed: 164.5ms\n",
      "\n",
      " Results saved to results/notebook_eval_20250814_233152_20250814_233154\n",
      "\n",
      "Completed in 5.4 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_evaluation(suite_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Run the complete evaluation suite\"\"\"\n",
    "    \n",
    "    print(f\"Starting eval: {suite_config['name']}\")\n",
    "    print(f\"Device: {suite_config['device']}\")\n",
    "    print(f\"Model: {suite_config['adapter_config']['model_name']}\")\n",
    "    print(f\"Trials: {suite_config['run']['repeats']}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = create_unique_output_dir(suite_config['name'])\n",
    "\n",
    "    try:\n",
    "        # Create adapter and runner\n",
    "        print(\"\\nLoading adapter...\")\n",
    "        adapter = get_adapter(suite_config)\n",
    "        \n",
    "        print(\"\\nCreating runner...\")\n",
    "        runner = LocalAgentRunner(adapter, suite_config['run']['sample_rate_hz'])\n",
    "        \n",
    "        # Run the suite\n",
    "        print(\"\\n⚡ Running evaluation suite...\")\n",
    "        start_time = time.time()\n",
    "        results = runner.run_suite(suite_config, output_dir)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\nCompleted in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"output_dir\": output_dir,\n",
    "            \"duration\": end_time - start_time\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nEvaluation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_result = run_evaluation(suite_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "Display the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary\n",
      "Output: results/notebook_eval_20250814_222747_20250814_223247\n",
      "Duration: 5.4s\n",
      "\n",
      "==================================================\n",
      "\n",
      "Task: vlm_caption_task\n",
      "Trials completed: 5\n",
      "Timing (ms): avg=155.9, min=145.8, max=168.2\n",
      "Memory: 15111.7 MB RAM, 15111.7 MB VRAM\n",
      "Utilization: 27.2% CPU, 19.2% GPU\n"
     ]
    }
   ],
   "source": [
    "def display_results(evaluation_result: Dict[str, Any]):\n",
    "\n",
    "    results = evaluation_result[\"results\"]\n",
    "    output_dir = evaluation_result[\"output_dir\"]\n",
    "    \n",
    "    print(f\"Results Summary\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    print(f\"Duration: {evaluation_result['duration']:.1f}s\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    for task_id, task_results in results.items():\n",
    "        print(f\"\\nTask: {task_id}\")\n",
    "        print(f\"Trials completed: {len(task_results)}\")\n",
    "        \n",
    "        if task_results:\n",
    "            timings = [trial.get('timing', {}).get('total_ms', 0) for trial in task_results]\n",
    "            if timings:\n",
    "                avg_time = sum(timings) / len(timings)\n",
    "                min_time = min(timings)\n",
    "                max_time = max(timings)\n",
    "                print(f\"Timing (ms): avg={avg_time:.1f}, min={min_time:.1f}, max={max_time:.1f}\")\n",
    "            \n",
    "            if 'resources' in task_results[0]:\n",
    "                resources = task_results[0]['resources']\n",
    "                if 'memory' in resources:\n",
    "                    mem = resources['memory']\n",
    "                    print(f\"Memory: {float(mem.get('ram_mb_peak', 'N/A')):.1f} MB RAM, {float(mem.get('vram_mb_peak', 'N/A')):.1f} MB VRAM\")\n",
    "                \n",
    "                if 'utilization' in resources:\n",
    "                    util = resources['utilization']\n",
    "                    print(f\"Utilization: {float(util.get('cpu_util_pct_avg', 'N/A')):.1f}% CPU, {float(util.get('gpu_util_pct_avg', 'N/A')):.1f}% GPU\")\n",
    "    \n",
    "# Display the results\n",
    "display_results(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    output_dir = create_unique_output_dir(suite_config['name'])\n",
    "    # Check if metrics were computed\n",
    "    metrics_file = output_dir / \"metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        print(\"\\nComputed Metrics:\")\n",
    "        with open(metrics_file) as f:\n",
    "            metrics = json.load(f)\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "        print(\"Saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
