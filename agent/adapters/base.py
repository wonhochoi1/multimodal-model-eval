# agent/adapters/base.py
from abc import ABC, abstractmethod
from typing import Any, Dict, Iterable, Optional, Callable, List

TokenCB = Callable[[str, float], None]           # (token, timestamp_s)
FrameCB = Callable[[Any, float], None]           # (frame_np, timestamp_s)
AudioCB = Callable[[bytes, float], None]         # (pcm_chunk, timestamp_s)

class Adapter(ABC):
    name: str
    modalities: List[str]  # e.g., ["image","video","audio","text"]

    @abstractmethod
    def load(self, config: Dict[str, Any]) -> None: ...
    def warmup(self) -> None: ...
    def teardown(self) -> None: ...

    # -------- Inference APIs --------
    # Image understanding/generation
    def infer_image(self, batch: List[Any], **kw) -> Dict: ...
    # Video: understanding or prediction; can stream first frame
    def infer_video(self, frames: Iterable[Any], on_first_frame: Optional[FrameCB]=None, **kw) -> Dict: ...
    # Audio: ASR/TTS; can stream first phoneme/audio chunk
    def infer_audio(self, samples: Any, on_first_audio: Optional[AudioCB]=None, **kw) -> Dict: ...
    # VLM: text generation w/ streaming tokens (for TTFT)
    def vlm_generate(self, prompt: str, media: Dict[str,Any], stream: bool,
                     on_token: Optional[TokenCB]=None, **kw) -> Dict: ...

    # -------- Encoding taps (for “information captured”) --------
    def encode_image(self, img) -> Dict: ...
    def decode_image(self, code) -> Any: ...
    def encode_video_frame(self, frame) -> Dict: ...
    def decode_video_frame(self, code) -> Any: ...
    def latent_entropy(self, codes: Iterable[Any]) -> float: ...
